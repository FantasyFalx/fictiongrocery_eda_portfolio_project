{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiction Grocery EDA Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello my name is George Stimson, a data scientist/analyst who loves to use my \n",
    "skills in mathematics and programming to help businesses make better decisions \n",
    "to reach their goals. Today I will walk you through an exploratory data analysis \n",
    "report I conducted for a grocery store chain called FictionGrocery. \n",
    "\n",
    "Ownership of a national grocery chain called FictionGrocery wanted to increase \n",
    "revenue in order to improve business operations. The company \n",
    "retained me to conduct a thorough analysis of their sales data, develop insights \n",
    "into their revenue stream, and provide recommendations to increase net sales. \n",
    "I received sales data that included unique sales id, branch and city locations, \n",
    "customer types, customer gender, product name and category, \n",
    "quantity of products sold, sales tax associated with each sale, and reward \n",
    "points for members of the store.\n",
    "\n",
    "To provide ownership with useful recommendations, I used python and its key \n",
    "libraries to analyze and visualize their sales data. For the core analysis \n",
    "I used Numpy and Pandas to clean the data, engineer new features, and to extract \n",
    "descriptive statistics. For data visualization I used Matplotlib and Seaborn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Section 1: Initial Exploration and Data Cleaning](#section-1:-initial-exploration-and-data-cleaning)\n",
    "    - [Section 1.1: Initial Exploration](#section-1.1:-initial-exploration)\n",
    "    - [Section 1.2: Data Cleaning](#section-1.2:-data-cleaning)\n",
    "- [Section 2: Feature engineering](#section-2:-feature-engineering)\n",
    "- [Section 3: Data Visualizations](#section-3:-data-visualizations)\n",
    "    - [Section 3.1: Univariate Analysis](#section-3.1:-univariate-analysis)\n",
    "    - [Section 3.2: Bivariate Analysis](#section-3.2:-bivariate-analysis)\n",
    "- [Section 4: Recommendations and Conclusion](#section-4-recommendations-and-conlcusion)\n",
    "    - [Section 4.1: Recommendations](#section-4.1:-recommendations)\n",
    "    - [Section 4.2: Conclusion](#section-4.2:-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1: Initial Exploration and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1.1: Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to conduct two steps to start our analysis:\n",
    " - Step 1: Import the required libraries.\n",
    " - Step 2: Load the provided data into a pandas dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = pd.read_csv(\"fiction_grocery_sales.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial steps are now complete. Let us kick off our exploration by checking \n",
    "out the data's dimensions, types, count of duplicated records, and its first 10 \n",
    "records. This will give us a clearer picture of the current state and schema of \n",
    "the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_characteristics = (\n",
    "    f\"Data Dimensions: {sales_data.shape}\\n\",\n",
    "    f\"Duplicate Record Count: {sales_data.duplicated().sum()}\\n\",\n",
    "    f\"Data Types: \\n{sales_data.dtypes}\",\n",
    ")\n",
    "\n",
    "for characteristic in range(len(data_characteristics)):\n",
    "    print(data_characteristics[characteristic])\n",
    "\n",
    "print(\"\\nSample Set:\\n\")\n",
    "sales_data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Some basic characteristics for the table are listed as follows:\n",
    "- The dataset has 1000 records and 13 columns.\n",
    "- No duplicated records are present in the data set\n",
    "- 7 columns are categorical while the remanining 6 are numerical. All categorical \n",
    "columns are objects, while the numerical columns are mostly floating point values\n",
    "**with one exception of the sale_id column which consist of integers**. \n",
    "- Nulls are present in some of the numerical columns. \n",
    "\n",
    "With this information, we can now develop a plan for cleaning FictionGroceries\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1.2: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning Plan:\n",
    "- Step 1: Convert columns to proper data types.\n",
    "- Step 2: Rename and drop unnecessary columns.\n",
    "- Step 3: Visualize product counts and distributions for all numerical columns.\n",
    "- step 4: Visualize null counts present in each column.\n",
    "- Step 5: Visualize distributions with nulls to determine how to fill null values.\n",
    "- Step 6: Visualize previous null count and new null count after removal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For step 1 we will convert all categorical variables from object to string type, numerical types from only floating point numbers to either floating points or integers, and the date column to a datetime object. This will ease up the analysis process in later steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = sales_data.convert_dtypes()\n",
    "sales_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For step two in cleaning the data let's start by dropping the sale_id and product \n",
    "category column.\n",
    "We will drop the sale_id column since there is no associated customer name \n",
    "column, \n",
    "and the values are in order from 1 to the max amount of records. \n",
    "We will drop the product category column since the right categories are not \n",
    "associated with the products. Examples include the shampoo product under the \n",
    "fruits category. \n",
    "This can be seen when we viewed the 1st 10 records of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = sales_data.drop(columns=[\"sale_id\",\"product_category\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of easier data access let’s rename some columns. We will rename the product_name column to products, city_names to cities, and sale_date to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = {\n",
    "    \"product_name\": \"products\",\n",
    "    \"total_price\": \"total\",\n",
    "    \"sale_data\": \"date\"\n",
    "}\n",
    "sales_data.rename(columns=new_names, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon the completion of step 1 and 2 of our cleaning process, we can move onto step 3. Before we visualize our information let's organize our stores and branches into a map for easier data access. This will let us extract insights on a per-city and per-branch basis. Since only 2 branches and 3 cities are present \n",
    "we will end up with a 6 key map which stores sub dataframes associated with the branch and city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_names = tuple(sales_data.products.unique())\n",
    "city_names = tuple(sales_data.city.unique())\n",
    "numerical_columns = tuple(sales_data.select_dtypes(include=\"number\").columns)\n",
    "branches = (\"A\",\"B\")\n",
    "store_map = {}\n",
    "\n",
    "for city in city_names:\n",
    "    for branch in branches:\n",
    "        key = f\"{city} {branch}\"\n",
    "        record_filter = (\n",
    "            (sales_data.city == city) & (sales_data.branch == branch)\n",
    "        )\n",
    "        store_map[key] = sales_data.loc[record_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our store data organized, let's visualize the total product count for each \n",
    "store and branch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(store_map.keys())\n",
    "counts = [int(store_map[key].value_counts().sum()) for key in keys]\n",
    "\n",
    "def product_count_visualizer(stores, data):\n",
    "    COLORS = (\"#008080\", \"#228B22\", \"#191970\", \"#0047AB\", \"#2A5D50\", \"#21421E\")\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(stores, data, color=COLORS)\n",
    "    ax.tick_params(axis=\"x\", labelsize=7)\n",
    "    ax.set_title(\"Total Product Counts for Stores\")\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_count_visualizer(keys, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this visual we can see that some branches don’t offer any products at all. \n",
    "This would mean that the branch column holds no significance in our analysis. \n",
    "We can drop this column and shift our analysis to a per-city, per-product basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = sales_data.drop(\"branch\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping the branch column let's take a look at our datasets dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sales Data Dimensions: {sales_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a 9 column 1000 record dataset. With some preliminary cleaning \n",
    "complete, let us now move onto step 4. We will first visualize the null counts \n",
    "in the dataset to determine which columns contain these values, then visualize \n",
    "the distributions for each numerical column \n",
    "on a per-city, per-product basis. These visuals will give us a better idea on \n",
    "how to fill the present null values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tuple(sales_data.columns)\n",
    "values = tuple(sales_data.isnull().sum())\n",
    "\n",
    "def null_count_visualizer(columns, data):\n",
    "    \n",
    "    COLORS = (\n",
    "    \"#003366\", \"#004466\", \"#005577\", \"#006666\", \"#007755\",\n",
    "    \"#118866\", \"#229977\", \"#33AA88\", \"#447799\", \"#335588\",\n",
    "    \"#223366\", \"#112244\"\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(data, columns, color=COLORS)\n",
    "    ax.tick_params(labelsize=10)\n",
    "    ax.set_title(\"Null Counts in FictionGrocery Data Set\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count_visualizer(values, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this bar chart we can see that all nulls are present in all numerical \n",
    "columns. Since these columns will be the core of our analysis lets \n",
    "visualize the distributions for each city to determine how we will fill these \n",
    "values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to output distributions of data for each city\n",
    "def product_distribution_visualizer(cities: tuple, columns: tuple) -> None:\n",
    "    \"\"\"Ouputs data distributions for product metrics on a per city basis.\n",
    "    \n",
    "    Arguments:\n",
    "    cities -- tuple of city names\n",
    "    columns -- tuple of column names\n",
    "    \"\"\"\n",
    "    LABELS=[\"Shampoo\", \"Apple\", \"Orange Juice\", \"Detergent\", \"Notebook\"]\n",
    "\n",
    "    fig, ax = plt.subplots(3,5, figsize=(17,17))\n",
    "    for i in range(len(cities)):\n",
    "        for j in range(len(columns)):\n",
    "            city = cities[i]\n",
    "            column = columns[j]\n",
    "            record_filter = sales_data.city == city\n",
    "            sub_set = sales_data.loc[record_filter, [\"products\",column]]\n",
    "            sns.kdeplot(sub_set, x=column, hue=\"products\", ax=ax[i, j], fill=True)\n",
    "            ax[i, j].set_xlabel(f\"{city} {column} product distributions\")\n",
    "            ax[i, j].legend(loc=\"upper right\", labels=LABELS)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_distribution_visualizer(city_names, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these visuals we can see that certain columns share similar distribution \n",
    "structures. Columns associated with **unit_price and quantity** contain a mix of \n",
    "distributions ranging from normal to somewhat bimodal. Columns associated with \n",
    "**tax, total, and reward_points** display a right skew. These patterns persist in all \n",
    "of FictionGroceries locations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These patterns can be due to a number of reasons such as presence of nulls, outliers \n",
    ",and customer purchasing behavior. To recenter these distributions\n",
    "we will fill the present null values associated with these columns with a metric. \n",
    "For the columns **unit_price and quantity** we will fill the null values with the \n",
    "**mean of their data**. For the **tax, total, and reward_points** columns we will fill \n",
    "the nulls with **the data’s median** due to the skew of the data. \n",
    "\n",
    "To complete this task we will create a function that fills the nulls in \n",
    "each column with the chosen metrics. Once this function is complete we will then\n",
    "create another function that visualizes the null counts before and after the\n",
    "null-filling operation. This will help verify that our null-filling\n",
    "process was completed properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_filler(cities: tuple, columns: tuple, products: tuple) -> None:\n",
    "    \"\"\"Function that fills nulls on per-city, per-product basis.\n",
    "    \n",
    "    Arguments:\n",
    "    cities -- tuple of city names\n",
    "    columns -- tuple of column names\n",
    "    products -- tuple of product names\n",
    "    \"\"\" \n",
    "    GROUP_1 = {\"unit_price\", \"quantity\"}\n",
    "    GROUP_2 = {\"tax\", \"total\", \"reward_points\"}\n",
    "    for city in cities:\n",
    "            for product in products:\n",
    "                for column in columns:\n",
    "            \n",
    "                    filter_1 = (\n",
    "                                (sales_data.city == city) \n",
    "                                & (sales_data.products == product) \n",
    "                            )\n",
    "                    filter_2 = (filter_1 & sales_data[column].isnull())\n",
    "                    \n",
    "                    if column in GROUP_1:\n",
    "                        mean = int(sales_data.loc[filter_1, column].mean())\n",
    "                        sales_data.loc[filter_2, column] = mean\n",
    "                    elif column in GROUP_2:\n",
    "                        median = int(sales_data.loc[filter_1, column].median())\n",
    "                        sales_data.loc[filter_2, column] = median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_filler_visualizer() -> None:\n",
    "    \"\"\"Function which visualizes null count for pre and post null removal.\"\"\"\n",
    "    null_count = sales_data.isnull().sum().sum()\n",
    "    non_null_count = len(sales_data) - null_count\n",
    "    LABELS = \"null count\", \"non null count\"\n",
    "    COLORS = (\"#F44336\", \"#4CAF50\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].pie([null_count, non_null_count], labels=LABELS, autopct=\"%1.1f%%\",\n",
    "              shadow={'ox': -0.04, 'edgecolor': 'none', 'shade': 0.9},\n",
    "              colors=COLORS)\n",
    "    ax[0].set_title(\"Pre Null Replacement\")\n",
    "    null_filler(city_names, numerical_columns, product_names)\n",
    "    null_count = sales_data.isnull().sum().sum()\n",
    "    non_null_count = len(sales_data) - null_count\n",
    "    ax[1].pie([null_count, non_null_count], labels=LABELS, autopct=\"%1.1f%%\", \n",
    "              shadow={'ox': -0.04, 'edgecolor': 'none', 'shade': 0.9},\n",
    "              colors=COLORS)\n",
    "    ax[1].set_title(\"Post Null Replacement\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_filler_visualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see our function has successfully replaced the null \n",
    "values with the mean and median for each column grouping. Imputing these metrics \n",
    "into their respective groups helps with recentering the data while maintaining \n",
    "its original integrity. This adjustment allows us to extract key insights, since \n",
    "null values no longer have an influence on the distributions. This will allow us \n",
    "to provide quality business recommendations to FictionGroceries ownership. With \n",
    "our data cleaning complete we can now move onto feature engineering.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Feature Engineering    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plan for Feature Engineering\n",
    "\n",
    "Since our task is to provide helpful recommendations so FictionGrocery can \n",
    "increase total revenue, we need to add some features to the data. The current \n",
    "dataset provides no columns that display gross profit per sale, and net profit \n",
    "per sale. Adding these features will help us extract unseen information about \n",
    "their sales. To determine gross profit per sale, we will create a new column \n",
    "**called gross_profit_sales**. \n",
    "The values of this column will be the **unit_price column \n",
    "multiplied by the quantity column**. To determine net profit per sale we will \n",
    "create a column **called net_profit_sales**. The values in this column will be \n",
    "found by **subtracting the sum of the tax and reward_points columns from the \n",
    "gross_profit_sales column**.We sum the tax and reward_points column because \n",
    "we assume that one reward point is equal to 1 dollar of purchasing power in the \n",
    "store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_profit = sales_data[\"unit_price\"] * sales_data[\"quantity\"]\n",
    "sales_data = sales_data.assign(gross_profit_sales=gross_profit)\n",
    "\n",
    "net_profit = (\n",
    "    sales_data[\"gross_profit_sales\"] \n",
    "    - (sales_data[\"tax\"] + sales_data[\"reward_points\"])\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "sales_data = sales_data.assign(net_profit_sales=net_profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we created our new columns, let's verify they are in the \n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above output we can verify our new features have been added to the data set. \n",
    "The new features of gross/net sales will allow us to uncover new sales insights for FictionGroceries operations. Some examples of these insights are the purchasing behavior of customers, seasonal trends in sales for certain products, and highest to lowest performing stores. With these features added let’s delve into the visual analysis of FictionGroceries sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Data Visualizations\n",
    "Upon completing feature engineering, we can now extract new insights in the data \n",
    "with the aid of visuals. This section will perform univariate and bivariate \n",
    "analysis on the data. The univariate analysis will visualize the contribution \n",
    "of all products to total net and gross profit per store. The bivariate analysis \n",
    "will visualize the count of customer type based on gender per store, the \n",
    "contribution of customer type to total net profit per store, and the correlations \n",
    "between sales metrics for all stores. These visuals will aid us in crafting \n",
    "ideal business recommendations for FictionGroceries management. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Section 3.1: Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct our univariate analysis for FictionGrocery, we will start by \n",
    "visualizing the contribution of each product to total gross and net profit \n",
    "per store. These contributions will be visualized using a pie chart. A pie chart \n",
    "will provide a clear picture of the percentage contribution each product has on total profitability per store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gross_and_net_contribution_extractor(cities: tuple, products: tuple) -> dict:\n",
    "    profit_data = {}\n",
    "    gross_profit_contributions = {}\n",
    "    net_profit_contributions = {}\n",
    "    gross_numbers = []\n",
    "    net_numbers = []\n",
    "    for city in cities:\n",
    "        for product in products:\n",
    "            record_filter_1 = ((sales_data.city == city)\n",
    "                             &(sales_data.products == product))\n",
    "            record_filter_2 = ((sales_data.city == city)\n",
    "                             &(sales_data.products == product))\n",
    "            \n",
    "            gross = sales_data.loc[record_filter_1, \"gross_profit_sales\"].sum()\n",
    "            net = sales_data.loc[record_filter_2, \"net_profit_sales\"].sum()\n",
    "            \n",
    "            gross_numbers.append(gross)\n",
    "            net_numbers.append(net)\n",
    "        \n",
    "        gross_profit_contributions[f\"{city}\"] = gross_numbers\n",
    "        net_profit_contributions[f\"{city}\"] = net_numbers\n",
    "\n",
    "        gross_numbers = []\n",
    "        net_numbers = []\n",
    "\n",
    "    profit_data[\"gross\"] = gross_profit_contributions\n",
    "    profit_data[\"net\"] = net_profit_contributions\n",
    "\n",
    "    return profit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gross_and_net_contribution_visualizer(cities: tuple, products: tuple) -> None:\n",
    "\n",
    "    data = gross_and_net_contribution_extractor(cities, products)\n",
    "    COLORS = ('#48C9B0', '#7D3C98', '#C0392B', '#5DADE2', '#F39C12')\n",
    "    profit_data = []\n",
    "    \n",
    "    fig, ax = plt.subplots(2,3, figsize=(8,8))\n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            city = cities[j]\n",
    "            # Conditional serves the purpose of identifying gross and net profit\n",
    "            if i == 0:\n",
    "                profit_data = data[\"gross\"][city]\n",
    "                gross_profit = round(sum(profit_data),2)\n",
    "                ax[i,j].pie(profit_data, labels=products, \n",
    "                            colors=COLORS, autopct=\"%1.1f%%\", \n",
    "                            textprops={\"fontsize\": 6})\n",
    "                ax[i,j].set_title(\n",
    "                    f\"Gross Profit Contribution for {city}\", fontsize=8\n",
    "                )\n",
    "                ax[i,j].set_xlabel(\n",
    "                    f\"Gross Profit for {city}: ${gross_profit}\", fontsize=7\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                profit_data = data[\"net\"][city]\n",
    "                net_profit = round(sum(profit_data),2)\n",
    "                ax[i,j].pie(profit_data, labels=products, colors=COLORS, \n",
    "                autopct=\"%1.1f%%\", textprops={\"fontsize\": 6})\n",
    "                ax[i,j].set_title(\n",
    "                    f\"Net Profit Contribution for {city}\", fontsize=8\n",
    "                )\n",
    "                ax[i,j].set_xlabel(\n",
    "                    f\"Net Profit for {city}: ${net_profit}\", fontsize=7\n",
    "                )\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(hspace=0) \n",
    "    fig.suptitle(\n",
    "        \"Gross and Net Profit Percentage contributions for Products Per Store\"\n",
    "    ) \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_and_net_contribution_visualizer(city_names, product_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see that the percentage contribution to total gross and net profit for FictionGroceries stores varies. Some products have similar percentage contributions, while others don’t. Shampoo has the highest percentage contribution to total gross and net profit for all stores. Its contribution ranges from 19% to 25%. Apples is the only product that has an almost uniform percentage contribution per store. Its contribution ranges from 15% to 16%, and it’s also the lowest contributor to total gross and net profit for all stores. All other products vary in their contribution percentages. \n",
    "\n",
    "We can also identify which stores contribute the most to total gross and net profit of FictionGrocery. The store which contributes the highest overall gross and net profit is the Chicago location, while the store that contributes the lowest is the Los Angeles location. The variation and gross and net profit product contributions is very low. Variations occur in the ranges of tenth of a percent. This is because the only expenses given to us in this data set is the sales tax and rewards points which only applies to customers with member status. If more expense data was present we would be able to calculate more accurate gross and net profit metrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Section 3.2: Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completion of our univariate analysis, we can now transition to conduct bivariate analysis on the provided data. We will visualize the count of member types based on gender in all of FictionGroceries locations, the contribution of each customer type to total net profit per store, and the correlations between all metrics that contribute to product sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we will visualize customer type based on gender for each store. We will use a count plot to visualize these relationships. This visual will give us a clear picture of the demographic breakdown of our customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gender_member_count_visualizer(cities: tuple) -> None:\n",
    "    COLORS = [\"#4CAF50\", \"#2196F3\"]\n",
    "    fig, ax = plt.subplots(3, figsize=(8,8))\n",
    "    print(\"Failed\")\n",
    "    for i in range(len(cities)):\n",
    "        city = cities[i]\n",
    "        record_filter = (sales_data.city == city)\n",
    "        data = sales_data.loc[record_filter]\n",
    "        ax[i].set_xlabel(f\"{city} Customer Status Gender Distribution per Store\")\n",
    "        sns.countplot(data,x=\"gender\",hue=\"customer_type\", ax=ax[i], palette=COLORS)\n",
    "        ax[i].set_xticks([0,1])\n",
    "        ax[i].set_xticklabels([\"Male\", \"Female\"])\n",
    "        ax[i].legend([\"Member\", \"Normal\"])\n",
    "    fig.suptitle(\"Count of Customer Type for Gender Per Store\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_member_count_visualizer(city_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visual uncovers new insights about the demographics of FictionGroceries customers. Not taking store location into account we can identify an almost even split in member status regardless of gender. This macro view does not transition to a per store basis since each store location has a unique customer breakdown. The New York customer base tends to hold member status regardless of gender. Los Angeles male customers tend to hold non member status while women tend to hold member status. Chicago's customer base is an inverse of New York’s because all customers tend to hold non member status. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a clear understanding of customer demographics we can now visualize the contribution of customer type to total net profit per store. To visualize these contributions we will use a barplot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def member_status_profit_contribution_visualizer(cities: tuple) -> None:\n",
    "    COLORS = [\"#4CAF50\", \"#2196F3\"]\n",
    "    fig, ax = plt.subplots(1,3, figsize=(8,8))\n",
    "    for i in range(len(cities)):\n",
    "        COLORS = [\"#4CAF50\", \"#2196F3\"]\n",
    "        city = cities[i]\n",
    "        record_filter = (sales_data.city == city)\n",
    "        data = sales_data.loc[record_filter]\n",
    "        sns.barplot(data, x=\"customer_type\", y=\"net_profit_sales\", \n",
    "                    estimator=\"sum\", errorbar=None, ax=ax[i], \n",
    "                    hue=\"customer_type\", palette=COLORS)\n",
    "        ax[i].set_xlabel(f\"{city}\")\n",
    "        ax[i].set_ylabel(\"Net Profit Sales\")\n",
    "        ax[i].set_xticks([0,1])\n",
    "        ax[i].set_xticklabels([\"Member\", \"Normal\"])\n",
    "    fig.suptitle(\"Net Profit Sales Contribution for Customer Type Per Store\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "        # Extract net profit\n",
    "        # display net profit per store based on a stacked bar chart\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_status_profit_contribution_visualizer(city_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visual gives us a clear picture on what customer type contributes the most to total profit for each store. Customers with member status in the New York and Los Angeles locations contribute the most to total net profit, while customers in the Chicago location with non member status are the highest contributors. This is interesting since it somewhat mirrors the demographic distributions, hinting at a possible relationship between customer demographics and profit contributions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a clear understanding of which customer type contributed the most to total net profits per store, we can now visualize the relationships between columns associated with product sales. To visualize these relationships we will use a heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_metric_correlation_visualizer(target_columns: tuple) -> None:\n",
    "    columns = list(target_columns)\n",
    "    numerical_data = sales_data[columns]\n",
    "    corr = numerical_data.corr()\n",
    "    sns.heatmap(corr, annot=True, cmap=\"crest\")\n",
    "    plt.title(\"Correlation Coefficients for Columns Associated with Product Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_metric_correlation_visualizer(numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visual provides us with some key insights for correlations between columns associated with product sales. The columns which have the highest correlations are the tax and total column with an r value of 0.95. This makes sense since higher purchase amounts lead to higher sales tax. \n",
    "\n",
    "Another key insight is the correlation value for the columns tax/ reward_points and total/ reward_points. The r-value for these column pairs is approximately 0.55, which shows a somewhat strong correlation. This correlation could lead to insights into the purchasing behavior of FictionGroceries customers who have member status. Members who belong to other stores utilize reward points to obtain a discount on their purchase total for the sake of saving money. When purchase totals are high sales tax tends to increase regardless of the presence of reward points. This is not proven for FictionGroceries customer base, in order to prove this possible connection additional testing will be required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon the completion of our visuals for univariate and bivariate analysis, we can now move to the next section. In our next section we will provide FictionGroceries management with some business recommendations based on our findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Recommendations and Conlcusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Section 4.1: Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profit Increase Recommendations: \n",
    "- FictionGroceries Los Angeles location contributes the lowest to total net and gross profit.Reasons for this lack are not clear due to the limitations of the provided data. Some recommendations for  profitability are to increase stock in items that contribute the most to profitability, while reducing stock of items that contribute the least. This will save the store money, and lead to a possible higher turnover rate of products since more popular products are in stock. This remains an inference due to the limitations of the available data. \n",
    "\n",
    "Marketing Recommendations: \n",
    "- Based on our findings, customer status varies per store. To increase customer experience. FictionGroceries can conduct targeted ad campaigns at customers who contribute the most to the store's profitability. This means offering special promotions for customers who hold non member status vs offering more reward points for customers who hold member status. \n",
    "\t\n",
    "Modeling Recommendations:\n",
    "- To uncover deeper insights within their data, we recommend FictionGroceries management develop predictive models to better understand customer purchasing behavior. These models will offer deeper insights than basic data analysis. Possible modeling approaches that could be utilized are regression for purchasing patterns, time series modeling for seasonal trends, and ensemble methods to predict membership signups. To conduct modeling more data will be required in the future. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Section 4.2: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this exploratory data analysis project, we completed several key steps. We first cleaned and optimized the dataset provided to us by FictionGroceries management. In our data cleaning section we converted data types to their proper format, and filled in missing values. We then added new features to the dataset such as gross and net profit per sale. With these new features we conducted univariate and bivariate analysis with the aid of visuals. With new insights extracted from our analysis we then provided ownership with recommendations on how to further improve and explore their business. This concludes the exploratory data analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
